---
title: " Detection of Knee Pathologies from MRI Using Deep Learning Models (CNNs) - Spring 2026 "
author: "Bala Raju Pidatala & Sabna BalasubramoniPillai (Advisor: Dr. Archaf Cohen)"
date: today
format:
  html:
    theme: flatly
    toc: true
    toc-depth: 2
    code-fold: true
    link-external-newwindow: true
    css: styles.css
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
link-citations: true
reference-location: document
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## 1 Introduction

The incidence of musculoskeletal disorders, especially those involving the knee, including osteoarthritis (OA), anterior cruciate ligament (ACL) injuries, and meniscal injuries, is a major global healthcare concern that has a profound effect on the mobility and quality of life of affected patients. Conventional diagnostic tools, especially X-ray radiography, have several limitations, including the inability to image soft tissues, often resulting in the missed diagnosis of early degenerative changes until late-stage damage has been incurred [@panwar2025early]. Magnetic Resonance Imaging (MRI) has recently become the standard diagnostic tool for musculoskeletal imaging because of its high contrast resolution and multi-planar imaging capabilities, which enable the detailed evaluation of ligaments, cartilage, and bone marrow [@qiu2021fusion]. However, the current manual analysis of knee MRI scans is a time-consuming and error-prone process, especially with regard to inter-observer variability and the high cognitive load of radiologists, which can result in delays in diagnosis and treatment [@bien2018deep].

Conventional MRI assessment of the knee relies on manual segmentation of anatomical structures, which is time-consuming and prone to variability. Zhou et al. (2018) demonstrated that a deep convolutional neural network can automatically segment knee joint tissues, including cartilage and meniscus, providing accurate and consistent delineation to support more efficient analysis of knee anatomy and pathology [@zhou2018deep]

The synergy of artificial intelligence, particularly deep learning (DL) and Convolutional Neural Networks (CNNs), provides a revolutionary approach to overcome these diagnostic hurdles by enabling an automated, objective, and scalable analysis of complex medical images [@oettl2025beyond]. CNNs are especially suited for medical imaging applications because they have the capability to automatically learn hierarchical feature representations directly from raw pixel data, including subtle pathological patterns that may not be visible to the human eye [@yeoh2021emergence]. Recent breakthroughs have shown the effectiveness of both 2D and 3D CNN models for segmenting musculoskeletal tissues and grading the severity of structural damage [@liu2018deep; @guida2021knee]. However, there are still major challenges to overcome in the current state of research, such as the requirement for large and high-quality annotated datasets and the lack of generalizability of models across different MRI protocols and hardware configurations [@goel2025federated].

To overcome these challenges, there is an urgent need for the development of efficient automated systems that are capable of multi-pathology detection and grading to help doctors in high-throughput settings [@astuto2021automatic]. Although specialized models have proved to be successful in specific tasks like the detection of ACL tears using self-supervised learning techniques [@aidarkhan2025self] or  staging meniscus degeneration [@pedoia20193d], the development of a comprehensive framework that can distinguish between different simultaneous knee injuries is still an intricate challenge. Using the latest advancements in CNN models, this research aims to develop an automated system for the detection and grading of knee pathologies from MRI images, which can eventually help in providing better patient care strategies for early intervention [@awan2021improved].

This research work is driven by the need to enhance the accuracy and efficiency of diagnosis in orthopedic radiology.

## 2 Methodology

Convolutional Neural Network (CNN)

A Convolutional Neural Network (CNN) is a deep learning model that has been specifically designed to work well with structured, grid-organized data, such as images. CNNs have been very successful in image processing and medical imaging tasks, as the network can learn the input images directly without the need to manually extract the features.

In the early layers of the network, the network learns the fundamental spatial features of the image, such as the gradients of the image or the boundaries of the structures. As the layers of the network get deeper, the network learns more complex and abstract features, such as the degeneration of the cartilage or the disruption of the fibers of the anterior cruciate ligament (ACL).

While 1D CNNs are often used to deal with sequential information and 2D CNNs are used to deal with information from individual image slices, 3D CNNs extend the operation of the convolutional layers to the volumetric space, including the dimension of depth in addition to height and width. This is particularly important in imaging modalities such as MRI, where spatial information across image slices is vital.

3D CNNs have demonstrated their ability to achieve better results in the detection and grading of ACL tears, meniscus degeneration, and the severity of osteoarthritis in knee MRI scans. This is due to the spatial information maintained across image slices, providing a deeper understanding of anatomy and placing 3D CNNs at the forefront of the latest artificial intelligence technology in the realm of cutting-edge orthopedics (Bien et al., 2018; Guida et al., 2021; Oettl et al., 2025).

In knee MRI applications, 3D CNNs have shown enhanced accuracy in detecting and grading ACL tears, meniscus degeneration, and osteoarthritis severity. By maintaining inter-slice spatial continuity, these models provide a richer anatomical understanding, positioning 3D CNNs as a powerful tool in cutting-edge orthopedic artificial intelligence systems (Bien et al., 2018; Guida et al., 2021; Oettl et al., 2025).


Basic Architecture of a CNN

 
A typical CNN consists of the following components:

![Basic Architecture of a CNN](Images/Cnn_Architecture.jpg)


1. Convolution Layer : 
Applies filters (kernels) to extract features from input images.

      Mathematical representation:
$$
Y(i,j) = \sum_{m}\sum_{n} \left[ X(i+m, j+n)\cdot W(m,n) \right] + b
$$


    Where:
	X= input image
	W= filter/kernel
	b= bias
	Y= output feature map
	
2. Activation Function:

    Commonly ReLU:
$$
f(x) = \max(0, x)
$$

3. Pooling Layer
Reduces spatial dimensions (e.g., Max Pooling).
4. Fully Connected Layer
Performs classification based on extracted features.
5. Output Layer
Uses Softmax (multi-class) or Sigmoid (binary/multi-label).

In medical imaging, CNNs have demonstrated strong performance in MRI-based diagnosis and tissue segmentation (Zhou et al. 2018; Liu et al. 2018).


Types of CNN:

CNNs are categorized based on the dimensionality of the input data and the sliding direction of the kernel:

| Type   | Input Data Examples | Kernel Movement |
|--------|--------------------|-----------------|
| 1D CNN | Time-series, audio, ECG signals (Ige & Sibiya, 2024). | Slides along one dimension (time/sequence). |
| 2D CNN | Grayscale/RGB images, medical X-rays (Taye, 2023). | Slides along two dimensions (height and width). |
| 3D CNN | Videos, MRI/CT scans, 3D point clouds (Guida et al., 2021). | Slides along three dimensions (height, width, and depth). |


3D Convolutional Neural Networks:

3D CNNs extend the capabilities of 2D networks by adding a third dimension (depth or time) to the convolution operation (Ige & Sibiya, 2024). This allows the network to capture spatiotemporal or volumetric contexts that are often lost when processing 3D data as independent 2D slices (Baheti et al., 2023; Guida et al., 2021).This characteristic makes 3D CNN particularly well-suited for medical imaging modalities such as magnetic resonance imaging (MRI) and computed tomography (CT), where anatomical structures extend across multiple slices.

The 3D convolution is defined as:
  
$$
Y(i,j,k) = \sum_{m}\sum_{n}\sum_{p} X(i+m, j+n, k+p)\cdot W(m,n,p) + b
$$

Where:
	X= Input 3D MRI volume
	W= 3D convolution kernel
	b= Bias
	Y= Output feature map
	i,j,k= Spatial voxel indices
This operation extracts volumetric features across depth, height, and width.


Applications in Knee MRI Analysis

3D CNNs have demonstrated strong performance in musculoskeletal imaging, particularly in knee MRI analysis.

- Tissue Segmentation:

   Liu et al. (2018) and Zhou et al. (2018) have used deep CNN-based architectures to achieve the segmentation of knee      joint anatomy, which has demonstrated better accuracy in tissue segmentation..

- Meniscus and Cartilage Degeneration Detection:

  Pedoia et al. (2019) have used 3D CNN architectures to identify and stage the degenerative morphological changes in      meniscus and patellofemoral cartilage, which emphasizes the need to extract volumetric features.

- Osteoarthritis Classification:

  Guida et al. (2021) have suggested a model based on the 3D CNN technique for the classification of knee osteoarthritis   using MRI, showing that the performance is enhanced compared to the 2D technique. Similarly, the effectiveness of the    CNN technique has been shown in the classification of osteoarthritis severity assessment by Rani et al. (2024).

- ACL Tear Detection:

  Bien et al. (2018) suggested the MRNet deep learning technique for the diagnosis of knee MRI. Recently, more efficient   learning techniques such as self-supervised learning have been used in the detection of ACL tears using the MRI scan,    as suggested in the study by Aidarkhan et al. (2025).

- Multimodal and Federated Learning Approaches:

  New frameworks have been developed to combine 3D CNNs with federated and few-shot learning techniques to achieve better   generalization across institutions (Goel, 2025). Moreover, the contribution of AI-based multimodal systems to the        development of orthopedic diagnostics has been emphasized in the study done by Oettl et al.(2025)

Performance and Advantages:

3D CNNs provide significant benefits in medical imaging (Avesta et al., 2023; Guida et al., 2021):

- Volumetric Context:

	They extract features from adjacent slices, detecting biomarkers (like cartilage degradation) that may be invisible in   a single 2D image (Guida et al., 2021).
	
- Higher Accuracy: 

	Brain and knee imaging studies have established that the 3D model has higher dice scores and accuracy compared to the    2D and 2.5D approaches (Avesta et al., 2023; Guida et al., 2021).
	
- Efficiency in Convergence: 

	3D models can converge 20% to 40% faster during training than their 2D counterparts when dealing with volumetric data    (Avesta et al., 2023).

Limitations and Assumptions

Despite their power, 3D CNNs face specific challenges (Avesta et al., 2023; Baheti et al., 2023):

- Computational Cost: 

	3D models require significantly more computational memory (often up to 20 times more) compared to 2D models (Avesta et   al., 2023).
	
- Data Scarcity: 

  Unlike 2D CNNs, which have access to enormous pre-trained data sets like ImageNet, 3D CNNs may face a scarcity of        large-scale pre-trained models, which may affect the stability of the model due to the random weights used during        initialization (Baheti et al., 2023).


## 3 Analysis & Results:

### 3.1 Dataset Description:
The MRNet dataset, a publicly available set of knee MRI scans gathered by the School of Medicine at Stanford University, is used in this research. The dataset contains images gathered from clinical studies performed at the Stanford University Medical Center over an eleven-year period from 2001 to 2012.
The structure of the dataset is divided into a training and validation set, with each MRI study labeled for three different diagnoses: 

- The presence of any abnormality, 
- Tears of the anterior cruciate ligament (ACL),
- Meniscal injuries.

#### 3.1.2 Data Organization
The dataset has a standardized partition structure, with a train/ directory for model development and a valid/ directory for model evaluation. The diagnostic ground truth is provided in the form of two CSV files, namely train-acl.csv and valid-acl.csv, each containing the examination identifier and a binary diagnostic label, where 0 represents the absence of an ACL tear and 1 represents a confirmed ACL tear.

#### 3.1.3 Problem Formulation
The task of clinical diagnosis is posed as a binary classification problem, where the goal of the model is to distinguish between ACL-intact and ACL-compromised knee examinations. Mathematically, the binary classification problem can be stated as:

$$
y=\begin{cases}
0 & \text{No ACL tear present} \\ 
1 & \text{ACL tear present} 
\end{cases}
$$
This problem statement allows the construction of a supervised learning model that can be used to automatically identify ACL tears from multi-planar MRI images.

### 3.2 Dataset Visualization & Exploratory Analysis:

Exploratory Data Analysis (EDA) is a fundamental component of any machine learning model's pipeline. It is critical to understand the structure and composition of the data before moving forward to train any model. This section of the report outlines a detailed exploratory data analysis of the MRNet dataset, proposed by [@bien2018deep]. in 2018, which consists of knee MRI images from three different imaging planes, i.e., sagittal, coronal, and axial. Each of these imaging sessions is associated with a binary variable representing the ACL tear condition of the knee. The class balance, volumetric structure, imaging characteristics, and cross-plane imaging of the MRNet dataset have all been addressed in the following visualizations, which were critical to the decisions made in the subsequent sections of this report.

#### 3.2.1 Class Distribution

<details class="collapase">
<summary><b>Code</b></summary>

```python
class_counts = labels_df["diagnosis"].value_counts()

plt.figure(figsize=(6, 4))
ax = class_counts.plot(kind="bar",
                       color=["#4C9BE8", "#E8654C"],
                       edgecolor="white",
                       width=0.5)

# Add count labels on top of each bar
for bar in ax.patches:
    ax.text(
        bar.get_x() + bar.get_width() / 2,
        bar.get_height() + 1,
        str(int(bar.get_height())),
        ha="center", va="bottom", fontweight="bold"
    )

plt.title("Class Distribution – ACL Tear Labels", fontsize=13, fontweight="bold")
plt.xlabel("")
plt.ylabel("Number of Exams")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()
plt.savefig("figures/class_distribution.png", dpi=300)
```

</details>

<p>The bar chart describes the class distribution of the ACL tear labels in the data set. The horizontal axis indicates the two classes in the diagnostic results, namely "No ACL Tear" and "ACL Tear," while the vertical axis indicates the number of examinations in the data set. The data set contains a total of 1,130 examinations, with 922 (81.6%) labeled as "No ACL Tear" and 208 (18.4%) labeled as "ACL Tear." This is a significant imbalance between the two classes, with an approximate ratio of 4.4:1 in favor of the negative class. This imbalance is clinically realistic, since ACL tears are not as common in the general population as the converse; nonetheless, it is a significant methodological limitation in the application of supervised machine learning, since a model trained on such an unbalanced data set will likely develop a bias toward the majority class, thereby achieving high accuracy but at the expense of poor sensitivity for the clinically important minority class. This figure is important because it establishes the underlying data set characteristics that directly influence the model design, loss function, and evaluation metric used in the study.</p>

<center>
```{r}
#| echo: false
#| fig-cap: "Figure 3.1: Class distribution of ACL tear labels"
#| out-width: "70%"
#| fig-align: "center"
#| fig-cap-location: margin
knitr::include_graphics("figures/Class_Distribution.png")
```
</center>

#### 3.2.2 Sample Middle Slices from the Sagittal Plane

<details class="collapase">
<summary><b>Code</b></summary>

```python
# Visualise Sample MRI Slices (Sagittal Plane)
# Grab exam IDs for each class
positive_ids = labels_df[labels_df["label"] == 1]["exam_id"].values[:2]
negative_ids = labels_df[labels_df["label"] == 0]["exam_id"].values[:2]

# Combine them with their labels for the plot title
sample_exams = [(eid, "ACL Tear")    for eid in positive_ids] + \
               [(eid, "No ACL Tear") for eid in negative_ids]

fig, axes = plt.subplots(1, 4, figsize=(14, 4))

for ax, (exam_id, diagnosis) in zip(axes, sample_exams):

   
    npy_path = os.path.join(DATA_DIR, "train", "sagittal", f"{exam_id:0>4}.npy")
    volume = np.load(npy_path)           # shape: (S, H, W)

    # Pick the middle slice as the most representative view
    middle_idx = volume.shape[0] // 2
    slice_img  = volume[middle_idx]      # shape: (H, W)

    ax.imshow(slice_img, cmap="gray")
    ax.set_title(f"ID: {exam_id}\n{diagnosis}", fontsize=10, fontweight="bold",
                 color="#E8654C" if diagnosis == "ACL Tear" else "#4C9BE8")
    ax.axis("off")

fig.suptitle("Sample Middle Slices – Sagittal Plane", fontsize=13, fontweight="bold")
plt.tight_layout()
plt.show()
```

</details>


<p>The following figure depicts four sample middle sagittal plane MRI slice images from the dataset, showing instances of both positive and negative ACL tear classes. As shown, each image has been labeled with the patient ID and the associated label, with "ACL Tear" instances depicted in red and "No ACL Tear" instances depicted in blue, based on their associated IDs 1 and 18, and 0 and 2, respectively. As depicted, the images have been shown in grayscale, consistent with standard MRI acquisition procedures. As shown, there are discernible structural differences in the ACL-tear and no ACL-tear classes, with the ACL-tear classes showing a lack of the characteristic linear hypointense feature representing the ligament, while the no ACL-tear classes show a well-defined ligamentous feature. However, discernible variability in knee orientation, contrast, and soft tissue intensity may be observed among the four sample images, showing the inherent variability in clinical data. As shown, the variability among the samples demonstrates the complexity of the classification task, warranting the use of deep learning architectures that are capable of learning complex hierarchical representations without relying on feature engineering.</p>

<center>
```{r}
#| echo: false
#| fig-cap: "Figure 3.2: Sample Middle Slices"
#| out-width: "70%"
#| fig-align: "center"
knitr::include_graphics("figures/Sample Middle Slices.png")
```
</center>

#### 3.2.3 Distribution of Slice Counts per Examination

<details class="collapase">
<summary><b>Code</b></summary>

```python
# Slice Count Histogram

slice_counts = []

for exam_id in labels_df["exam_id"]:
    npy_path = os.path.join(DATA_DIR, "train", "sagittal", f"{exam_id:0>4}.npy")
    volume = np.load(npy_path)
    slice_counts.append(volume.shape[0])   # number of slices

labels_df["num_slices"] = slice_counts

plt.figure(figsize=(7, 4))
plt.hist(labels_df["num_slices"], bins=20, color="#4C9BE8", edgecolor="white")
plt.title("Distribution of Slice Counts (Sagittal Plane)", fontsize=13, fontweight="bold")
plt.xlabel("Number of Slices per Exam")
plt.ylabel("Number of Exams")
plt.tight_layout()
plt.show()

# Quick stats
print(f"Min slices : {labels_df['num_slices'].min()}")
print(f"Max slices : {labels_df['num_slices'].max()}")
print(f"Mean slices: {labels_df['num_slices'].mean():.1f}")
```

</details>


<p>The above histogram shows the distribution of the number of slices in the sagittal plane for each MRI exam. The horizontal axis shows the number of slices in each exam. This varies from around 15 slices to around 50 slices. The vertical axis shows the number of exams. The distribution of slices in the sagittal plane appears to be approximately unimodal. The main peak occurs at around 25 slices per exam, with approximately 150 exams. There is a smaller peak around 30-35 slices per exam, with around 100-110 exams. The distribution of slices in the sagittal plane appears skewed to the right. This means that a long tail of the distribution goes towards a maximum of 50 slices per exam. Here, the number of exams is less than 5. This variation in the number of slices in the sagittal plane arises due to the inherent variation in clinical images. This variation arises because different imaging protocols are followed in different hospitals and institutions. This figure is of great importance as it guides the way in which the volumetric inputs are standardized during the pre-processing stage.</p>

<center>

```{r}
#| echo: false
#| fig-cap: "Figure 3.3: Distribution of Slice Counts"
#| out-width: "70%"
#| fig-align: "center"
knitr::include_graphics("figures/Distribution of Slice Counts.png")
```
</center>

#### 3.2.4 Pixel Intensity

<details class="collapase">
<summary><b>Code</b></summary>

```python

# Pixel Intensity Histogram

# Only load a small sample to avoid slow runtimes
n_sample  = 10
sample_ids = labels_df["exam_id"].sample(n=n_sample, random_state=42).values

all_pixels = []

for exam_id in sample_ids:
    npy_path = os.path.join(DATA_DIR, "train", "sagittal", f"{exam_id:0>4}.npy")
    volume = np.load(npy_path).astype(np.float32)

    # Flatten volume and take a random subset of pixels (for speed)
    flat = volume.ravel()
    sample_px = flat[np.random.choice(len(flat), size=3000, replace=False)]
    all_pixels.append(sample_px)

all_pixels = np.concatenate(all_pixels)

plt.figure(figsize=(7, 4))
plt.hist(all_pixels, bins=60, color="#E8A84C", edgecolor="white")
plt.title(f"Pixel Intensity Distribution (sample of {n_sample} exams)",
          fontsize=13, fontweight="bold")
plt.xlabel("Pixel Intensity Value")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

print(f"Intensity range: {all_pixels.min():.1f} – {all_pixels.max():.1f}")
print(f"Mean intensity : {all_pixels.mean():.1f}")
```

</details>


<p>The histogram in the above image demonstrates the pixel intensity values for a random selection of ten images. The horizontal axis of the histogram ranges from values lower than 0 up to 255. The vertical axis of the histogram corresponds to the values’ frequencies. The values are highly skewed towards the right. The highest frequency values are concentrated in the lower intensity values and reach a peak of around 20-25. The frequency value in this region approaches 2000. After this peak, the values decrease gradually as the intensity increases. This demonstrates that the pixels in an MRI image are not uniformly distributed. The presence of a secondary peak of around 300 occurs when the intensity reaches its maximum value of 255. This could be due to the presence of pixels in images of joint effusion. This figure is useful as it justifies the normalization of all the images before they are fed into the model for training.</p>

<center>
```{r}
#| echo: false
#| fig-cap: "Figure 3.4: Pixel Intensity"
#| out-width: "70%"
#| fig-align: "center"
knitr::include_graphics("figures/Pixel Intensity.png")
```
</center>



## Conclusion
>>>>>>> 5164d87f1bb70bfdb0beed4f35ceeaa08c96889a


## References
